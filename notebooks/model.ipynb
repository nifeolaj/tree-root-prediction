{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "994de026-f4d4-4867-9da3-4f5710b2f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # For data handling\n",
    "import numpy as np  # For numerical operations\n",
    "import networkx as nx  # For graph algorithms\n",
    "import ast  # To safely parse edge list strings into Python lists\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "import seaborn as sns  # For advanced plots\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV  # Grouped cross-validation and hyperparameter tuning\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c84d849-8f20-4e2b-b63f-0a68281f1102",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# read training data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mdata/train.csv\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/BDMA /UPC/ML/Project/tree-root-prediction/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/BDMA /UPC/ML/Project/tree-root-prediction/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/BDMA /UPC/ML/Project/tree-root-prediction/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/BDMA /UPC/ML/Project/tree-root-prediction/.venv/lib/python3.13/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/BDMA /UPC/ML/Project/tree-root-prediction/.venv/lib/python3.13/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'data/train.csv'"
     ]
    }
   ],
   "source": [
    "# read training data\n",
    "df = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02caca81-f64f-4ae3-a41e-06feced9ced7",
   "metadata": {},
   "source": [
    "## Inspect the Data\n",
    "\n",
    "We inspect the train dataset to check the structure of the data and understand the dataset more. We check the summary statistics and look out for possible errors, missing values, outliers or duplicates. We also check the the datatypes of each variable and ensure they are of the proper type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9f5e0f-58fd-468b-925a-7b2099094ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6652e96-5255-4f8d-8648-16c0ddee57fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of rows and columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8f6e49-2ac0-472d-9bde-d3c11a3ef691",
   "metadata": {},
   "source": [
    "Target variable: root. \n",
    "It says the node that is the root. But to make the task simpler, we will turn it to a binary classification task. Such that, each row takes a node and the target variable will the is_root, where 0 will indicate that the node isn't the root and 1 indicates that it is the root.\n",
    "\n",
    "This will cause data imbalance in the dataset because most of the classes will be 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfd3053-f824-4e53-9f8b-127823da14b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check other variables in the data\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea32e6-01ea-4e01-a7a2-3ef2dfa911bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic info\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c3e269-6e77-47aa-93a6-84a09b92c0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5401a79c-6a95-4f68-abbe-0b1fd138047a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c070f10-463f-4d26-9ea5-e9db86188c2a",
   "metadata": {},
   "source": [
    "Training data contains no missing values or duplicated columns, which is good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54658308-9c5a-46cf-b3c9-c92671c74426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc932cd6-32fc-4cce-9c55-c991c5c5dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm the type of object\n",
    "type(df['edgelist'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb6bb8-3d7a-4cba-9ed3-715a9fc833a7",
   "metadata": {},
   "source": [
    "Edgelist is of the datatype object (string).\n",
    "\n",
    "It needs to be converted to a python edgelist which can be used to create the networkx tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff9fbc5-6c19-49e0-85f0-02d296be9a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9813e827-32e4-4f13-ac0f-04fff59fc0bf",
   "metadata": {},
   "source": [
    "There are sentences with 3 nodes (words) and some with as many as 70 nodes. This should be taken into consideration when normalizing. it will be advisable to normalize per sentence coz of this imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395f243-9bce-41b8-be4e-72cb3236d964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence length distribution\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(data=df, x='n', bins=30, kde=True)\n",
    "plt.title('Distribution of Tree Sizes (Number of Nodes)')\n",
    "plt.xlabel('Number of nodes')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f679966a-2f4e-4e26-8d68-e0ad371ef12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language distribution (check number of languages and number of sentences per language)\n",
    "lang = df['language'].nunique()\n",
    "print(f'There are {lang} languages. Each language has the following number of sentences:')\n",
    "df['language'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f84117-67b9-4b91-a3df-3ac611e5b289",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root node analysis\n",
    "print(df['root'].describe())\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.histplot(data=df, x='root', bins=30)\n",
    "plt.title('Distribution of Root Nodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd71c310-9662-4b09-b6dc-abbacc33c2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_tree(edges, root, title=\"\"):\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "    \n",
    "    pos = nx.planar_layout(G) if nx.is_planar(G) else nx.spring_layout(G)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    nx.draw(G, pos, with_labels=True, arrows=True, node_size=800, node_color='lightblue')\n",
    "    plt.title(f\"Tree Visualization (Root: {root}) {title}\")\n",
    "    plt.show()\n",
    "\n",
    "# Visualize first few trees\n",
    "for i, row in df.head(3).iterrows():\n",
    "    visualize_tree(ast.literal_eval(row['edgelist']), row['root'], f\"Language: {row['language']}, Sentence: {row['sentence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a839fc29",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe7e43-7a6c-4c54-b094-7c12d6df55f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    Enhanced preprocessing with additional features for tree root prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    df['edgelist'] = df['edgelist'].apply(ast.literal_eval)\n",
    "    \n",
    "    def enhanced_features(edgelist):\n",
    "        \"\"\"\n",
    "        Extract comprehensive features for tree root prediction\n",
    "        \"\"\"\n",
    "        T = nx.from_edgelist(edgelist)\n",
    "        \n",
    "        # Basic centrality measures (your existing ones)\n",
    "        dc = nx.degree_centrality(T)\n",
    "        cc = nx.harmonic_centrality(T)\n",
    "        bc = nx.betweenness_centrality(T)\n",
    "        pc = nx.pagerank(T)\n",
    "        clc = nx.closeness_centrality(T)\n",
    "        kz = nx.katz_centrality_numpy(T, alpha=0.005, beta=1.0)\n",
    "        lc = nx.load_centrality(T)\n",
    "        andc = nx.average_neighbor_degree(T)\n",
    "        \n",
    "        # NEW FEATURES FOR ROOT PREDICTION\n",
    "        \n",
    "        \n",
    "        # 2. Distance-based features\n",
    "        # Average distance to all other nodes\n",
    "        shortest_paths = dict(nx.all_pairs_shortest_path_length(T))\n",
    "        avg_distances = {}\n",
    "        sum_distances = {}\n",
    "        \n",
    "        for node in T.nodes():\n",
    "            distances = list(shortest_paths[node].values())\n",
    "            avg_distances[node] = np.mean(distances)\n",
    "            sum_distances[node] = sum(distances)\n",
    "        \n",
    "        # 3. Tree-specific features\n",
    "        # Distance from center(s) of the tree\n",
    "        center_nodes = nx.center(T)\n",
    "        periphery_nodes = nx.periphery(T)\n",
    "        \n",
    "        distance_from_center = {}\n",
    "        distance_from_periphery = {}\n",
    "       \n",
    "        for node in T.nodes():\n",
    "            # Distance to closest periphery\n",
    "            distance_from_periphery[node] = min(nx.shortest_path_length(T, node, periph) \n",
    "                                              for periph in periphery_nodes)\n",
    "      \n",
    "        # 4. Subtree size features\n",
    "        # For each node, compute size of subtree when that node is removed\n",
    "        subtree_sizes = {}\n",
    "        for node in T.nodes():\n",
    "            T_copy = T.copy()\n",
    "            T_copy.remove_node(node)\n",
    "            components = list(nx.connected_components(T_copy))\n",
    "            # Size of largest component when node is removed\n",
    "            subtree_sizes[node] = max(len(comp) for comp in components) if components else 0\n",
    "        \n",
    "        # 5. Neighbor-based features\n",
    "        neighbor_degrees = {}\n",
    "        neighbor_centralities = {}\n",
    "        second_order_neighbors = {}\n",
    "        \n",
    "        for node in T.nodes():\n",
    "            neighbors = list(T.neighbors(node))\n",
    "            if neighbors:\n",
    "                neighbor_degrees[node] = np.mean([T.degree(n) for n in neighbors])\n",
    "                neighbor_centralities[node] = np.mean([dc[n] for n in neighbors])\n",
    "                # Second-order neighbors (neighbors of neighbors)\n",
    "                second_neighbors = set()\n",
    "                for neighbor in neighbors:\n",
    "                    second_neighbors.update(T.neighbors(neighbor))\n",
    "                second_neighbors.discard(node)  # Remove self\n",
    "                second_order_neighbors[node] = len(second_neighbors)\n",
    "            else:\n",
    "                neighbor_degrees[node] = 0\n",
    "                neighbor_centralities[node] = 0\n",
    "                second_order_neighbors[node] = 0\n",
    "        \n",
    "        \n",
    "        # 7. Relative position features\n",
    "        # Node's degree relative to max degree\n",
    "        max_degree = max(dict(T.degree()).values())\n",
    "        relative_degree = {node: T.degree(node) / max_degree for node in T.nodes()}\n",
    "        \n",
    "        # Node's centrality relative to max centrality\n",
    "        max_dc = max(dc.values())\n",
    "        relative_centrality = {node: dc[node] / max_dc for node in T.nodes()}\n",
    "        \n",
    "        # 8. Tree depth features (if we can infer a root)\n",
    "        # Use the most central node as a proxy root for depth calculation\n",
    "        proxy_root = max(dc.keys(), key=lambda x: dc[x])\n",
    "        depths_from_proxy = nx.single_source_shortest_path_length(T, proxy_root)\n",
    "        \n",
    "        # 9. Bridge and articulation point features\n",
    "        bridges = list(nx.bridges(T))\n",
    "        articulation_points = set(nx.articulation_points(T))\n",
    "        \n",
    "        is_articulation = {node: 1 if node in articulation_points else 0 for node in T.nodes()}\n",
    "        bridge_count = {node: sum(1 for bridge in bridges if node in bridge) for node in T.nodes()}\n",
    "        \n",
    "        # 10. Statistical features within neighborhoods\n",
    "        local_degree_variance = {}\n",
    "        \n",
    "        for node in T.nodes():\n",
    "            neighbors = list(T.neighbors(node))\n",
    "            if len(neighbors) > 1:\n",
    "                neighbor_degrees_list = [T.degree(n) for n in neighbors]\n",
    "                local_degree_variance[node] = np.var(neighbor_degrees_list)\n",
    "            else:\n",
    "                local_degree_variance[node] = 0\n",
    "        \n",
    "        # Combine all features\n",
    "        features = {}\n",
    "        for node in T.nodes():\n",
    "            features[node] = (\n",
    "                # Original centrality features\n",
    "                dc[node], cc[node], bc[node], pc[node], clc[node], \n",
    "                kz[node], lc[node], andc[node],\n",
    "                \n",
    "                # New features\n",
    "                avg_distances[node],               # 10: Average distance to all nodes\n",
    "                sum_distances[node],               # 12: Sum of distances\n",
    "                distance_from_periphery[node],     # 14: Distance from periphery\n",
    "                subtree_sizes[node],               # 17: Size of largest subtree when removed\n",
    "                neighbor_degrees[node],            # 18: Average neighbor degree\n",
    "                neighbor_centralities[node],       # 19: Average neighbor centrality\n",
    "                second_order_neighbors[node],      # 20: Number of second-order neighbors\n",
    "                relative_degree[node],             # 22: Relative degree\n",
    "                relative_centrality[node],         # 23: Relative centrality\n",
    "                depths_from_proxy[node],           # 24: Depth from proxy root\n",
    "                is_articulation[node],             # 25: Is articulation point\n",
    "                bridge_count[node],                # 26: Number of bridges connected to\n",
    "                local_degree_variance[node],       # 28: Local degree variance\n",
    "                \n",
    "            )\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    df['centralities'] = df['edgelist'].apply(enhanced_features)\n",
    "    \n",
    "    def binary_classification(df):\n",
    "        \"\"\"\n",
    "        Convert to binary classification format with enhanced features\n",
    "        \"\"\"\n",
    "        records = []\n",
    "        has_root = 'root' in df.columns\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            for vertex, feature_tuple in row['centralities'].items():\n",
    "                record = {\n",
    "                    'language': row['language'],\n",
    "                    'sentence': row['sentence'],\n",
    "                    'n': row['n'],\n",
    "                    'vertex': vertex,\n",
    "                }\n",
    "                \n",
    "                # Add all features with descriptive names\n",
    "                feature_names = [\n",
    "                    'degree', 'harmonic', 'betweenness', 'pagerank', 'closeness',\n",
    "                    'katz', 'load', 'avg_neighbor_degree',\n",
    "                     'avg_distance', 'sum_distance',\n",
    "                    'distance_from_periphery', \n",
    "                    'subtree_size',\n",
    "                    'neighbor_degrees', 'neighbor_centralities', 'second_order_neighbors',\n",
    "                     'relative_degree', 'relative_centrality',\n",
    "                    'depth_from_proxy', 'is_articulation', 'bridge_count',\n",
    "                    'local_degree_variance'\n",
    "        \n",
    "                ]\n",
    "                \n",
    "                for i, feature_name in enumerate(feature_names):\n",
    "                    record[feature_name] = feature_tuple[i]\n",
    "                \n",
    "                if has_root:\n",
    "                    record['root'] = row['root']\n",
    "                records.append(record)\n",
    "        \n",
    "        binary_df = pd.DataFrame(records)\n",
    "        binary_df = binary_df.sort_values(['language', 'sentence', 'vertex']).reset_index(drop=True)\n",
    "        \n",
    "        if has_root:\n",
    "            binary_df['is_root'] = (binary_df['vertex'] == binary_df['root']).astype(int)\n",
    "        \n",
    "        return binary_df\n",
    "    \n",
    "    return binary_classification(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f7b847-4d30-40e1-bdd9-a32897be0cf9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = preprocess(df)\n",
    "\n",
    "# check with project guideline\n",
    "df[(df['language'] == 'Arabic') & (df['sentence'] == 62)]\n",
    "\n",
    "#n number of nodes, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75b83b-286a-43af-bce7-ad75a1240537",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check columns where all values are 0\n",
    "df.columns[(df == 0).all()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b636b47-0bdf-4297-8e13-ee439999179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Analyze Centrality Distributions\n",
    "plt.figure(figsize=(12,8))\n",
    "for feature in ['degree', 'harmonic', 'betweenness', 'pagerank']:\n",
    "    sns.kdeplot(df[feature], label=feature)\n",
    "plt.title('Centrality Distributions')\n",
    "plt.legend()\n",
    "plt.xlim(0, 1)  # Limit x-axis from 0 to 5\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52580dd1-8287-4ec8-a2e5-05ce6fe294cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop root column\n",
    "df = df.drop('root', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907983e9-8c76-4114-b441-4010049d8979",
   "metadata": {},
   "source": [
    "## Data Exploration of the Expanded Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038f834b-fdc6-4d43-878b-d1ea21e330b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save expanded data\n",
    "df.to_csv('binary_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f1aed7-5791-4750-9003-d51cb240a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('binary_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bbc8ee-fcac-442a-b50b-3a5fbc89644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fead61b-abc6-43b1-b9fd-5bc49f961094",
   "metadata": {},
   "source": [
    "- change language to categorical variable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a95f1dd-a720-4c80-8aff-8f3be650db56",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['language'] = train_df['language'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3ce287-716d-4da1-8187-182a5cb3cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get number of rows and columns\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41346f1-ad39-4645-bde2-ee96980c45f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirm there are no missing values\n",
    "train_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d5c5a1-730c-4866-b1f5-e190b4940ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "train_df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78be8203-1628-4e06-b60a-6e0574d39b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d3b623-477e-4a25-8e99-b8d15ac4a2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the carrelation between features\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.drop(['sentence', 'language'], axis=1).corr(), annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa8e104",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Drop non-numeric columns\n",
    "correlation_matrix = df.drop(['sentence', 'language'], axis=1).corr()\n",
    "\n",
    "# Display as table\n",
    "print(correlation_matrix.round(2))\n",
    "correlation_matrix.round(2).to_csv(\"correlation_matrix.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b861809-405e-4495-ba49-3975cd4a8457",
   "metadata": {},
   "source": [
    "## Resampling: Splitting Data into Train and Validation Set\n",
    "\n",
    "To estimate the generalization error, we split the data into train and validation set. We will then use a 5-fold cross validation over the train set so we train over a good sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f556cb-6554-42e1-9b78-979c90cb115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features and target value\n",
    "X = train_df.drop('is_root', axis=1)\n",
    "y = train_df['is_root']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1019597-a683-4cba-ae9e-14c34fb97b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to avoid data leakage use group split\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Grouping by sentence ID\n",
    "groups = train_df['sentence'] \n",
    "\n",
    "# Perform group split (80% train, 20% val)\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, val_idx = next(gss.split(X, y, groups=groups))\n",
    "\n",
    "X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "# Verify no sentence appears in both splits\n",
    "train_sentences = set(groups.iloc[train_idx])\n",
    "val_sentences = set(groups.iloc[val_idx])\n",
    "assert train_sentences.isdisjoint(val_sentences), \"Data leakage detected!\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4635792-f49e-401b-8694-5288a67ed549",
   "metadata": {},
   "source": [
    "## Per Sentence Normalization\n",
    "\n",
    "It will be better to normalize the data since it has different different ranges. we have to do a sentence level normalization because different sentences have different number of nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4367a9f4-e3f7-4ef0-98df-970529d47819",
   "metadata": {},
   "source": [
    "Note: Group by language then sentence coz per sentence will scale all the sentences from all the languages as one. however for example, Arabic and Turkish trees for the same sentence may have different structures, so we Normalize per-language AND per-sentence by grouping by both language and sentence. treating them as a single group would mix languages during normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d201439-eb09-4d06-acf4-c5edb75491b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "features = X_train.drop(['language', 'sentence', 'n', 'vertex'], axis=1).columns\n",
    "\n",
    "def scaling(X):\n",
    "    \"\"\"\n",
    "    Normalize features using MinMaxScaler\n",
    "    \"\"\"\n",
    "    normalized_ft = X.groupby(['language','sentence'])[features].transform(lambda x: MinMaxScaler().fit_transform(x.values.reshape(-1,1)).flatten())\n",
    "    X_ft= X.drop(features, axis=1) \n",
    "    normalized_X = pd.concat([X_ft, normalized_ft], axis=1)\n",
    "    \n",
    "    return normalized_X\n",
    "\n",
    "X_train_normalized = scaling(X_train)\n",
    "X_val_normalized = scaling(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de8a243-fb7c-4659-bfe0-6c2665579c41",
   "metadata": {},
   "source": [
    "we fit and transform the test set independently because the sentences are different in the two sets, so we can't use the scalers used for train to transform the test set, as we performed a within sentence normalization\n",
    "\n",
    "We also only performed normalization on the centrality metrics and on the number of nodes n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb5f7b1-49a0-4004-a0cc-f7340a1d577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns not useful\n",
    "X_train_normalized = X_train_normalized.drop(['language', 'sentence', 'vertex'], axis=1)\n",
    "X_val_normalized = X_val_normalized.drop(['language', 'sentence', 'vertex'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eef8c8",
   "metadata": {},
   "source": [
    "# Imbalance Handling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023ebc4",
   "metadata": {},
   "source": [
    "We tried undersmapling and oversampling the data but it was not as helpful, so it will remain commented here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e09884",
   "metadata": {},
   "source": [
    "## Undersampling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ac3bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import ast\n",
    "import networkx as nx\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to remove bottom 25% nodes by degree\n",
    "def prune_low_degree_nodes(edgelist_str, retain_percent=0.75):\n",
    "    edges = ast.literal_eval(edgelist_str)\n",
    "    G = nx.Graph(edges)\n",
    "\n",
    "    if len(G.nodes) == 0:\n",
    "        return edges  # nothing to do\n",
    "\n",
    "    # Sort nodes by degree\n",
    "    degrees = dict(G.degree())\n",
    "    sorted_nodes = sorted(degrees.items(), key=lambda x: x[1])\n",
    "    \n",
    "    # Keep top N% nodes\n",
    "    keep_n = int(len(sorted_nodes) * retain_percent)\n",
    "    keep_nodes = set([node for node, _ in sorted_nodes[-keep_n:]])\n",
    "    \n",
    "    # Filter edges\n",
    "    pruned_edges = [(u, v) for u, v in edges if u in keep_nodes and v in keep_nodes]\n",
    "    \n",
    "    return pruned_edges\n",
    "\n",
    "tqdm.pandas()\n",
    "X_train_normalized = X_train_normalized.copy()\n",
    "X_train_normalized['edgelist'] = X_train_normalized.progress_apply(\n",
    "    lambda row: prune_low_degree_nodes(row['edgelist']) if row['language'] == major_class else row['edgelist'],\n",
    "    axis=1\n",
    ")\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cf2faa",
   "metadata": {},
   "source": [
    "## Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13695ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Separate features and target\n",
    "X_features = X_train_normalized.drop(columns=['target', 'language', 'sentence', 'node', 'group'])  # Keep only numerical features\n",
    "y_target = train_df['target']\n",
    "\n",
    "# Save identifier columns for later merge\n",
    "identifier_cols = train_df[['language', 'sentence', 'node']].reset_index(drop=True)\n",
    "\n",
    "# Step 2: Apply RandomOverSampler to balance the minority class (target == 1)\n",
    "ros = RandomOverSampler(sampling_strategy='minority', random_state=42)\n",
    "X_balanced, y_balanced = ros.fit_resample(X_features, y_target)\n",
    "\n",
    "# Step 3: Reattach metadata using sample indices\n",
    "resampled_ids = identifier_cols.iloc[ros.sample_indices_].reset_index(drop=True)\n",
    "resampled_data = pd.concat([\n",
    "    resampled_ids,\n",
    "    pd.DataFrame(X_balanced, columns=X_features.columns),\n",
    "    pd.Series(y_balanced, name='target')\n",
    "], axis=1)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabeaa27-ff39-4937-9dce-438df8d13a2b",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485c4a17-6622-4ca0-b33e-b095a29c6092",
   "metadata": {},
   "source": [
    "### Linear Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b622746-ebf7-454f-abb4-e657812b73e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "lg_model = LogisticRegression(class_weight='balanced', random_state=42, max_iter=1000)\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1],             # Regularization strength\n",
    "    'penalty': ['l2'],                  # l1 requires solver='liblinear' or 'saga'\n",
    "    'solver': ['lbfgs', 'liblinear'],                # 'lbfgs' supports l2 and multiclass\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lg_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=GroupKFold(n_splits=5),\n",
    "    scoring='f1',   \n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_normalized, y_train, groups=X_train['sentence'])\n",
    "\n",
    "\n",
    "# Train\n",
    "logreg = grid_search.best_estimator_\n",
    "\n",
    "# Predict\n",
    "y_pred = logreg.predict(X_val_normalized)\n",
    "y_probs = logreg.predict_proba(X_val_normalized)[:, 1]  # Probabilities for root class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb60846a-c816-41c4-bf95-3f6a25240806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Test F1 Score:\", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca512bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "param_grid = {\n",
    "    'shrinkage': [None, 'auto'],  # You can also try float values with solver='lsqr'\n",
    "    'solver': ['svd', 'lsqr']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=lda,\n",
    "    param_grid=param_grid,\n",
    "    cv=GroupKFold(n_splits=5),\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_normalized, y_train, groups=X_train['sentence'])\n",
    "\n",
    "# Best model\n",
    "lda_best = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred = lda_best.predict(X_val_normalized)\n",
    "y_probs = lda_best.predict_proba(X_val_normalized)[:, 1]  # Probability of positive class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbba3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Evaluation for LDA\n",
    "print(\"📌 Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "print(\"\\n📊 Classification Report (LDA):\")\n",
    "print(classification_report(y_val, y_pred, digits=4))\n",
    "\n",
    "f1 = f1_score(y_val, y_pred)\n",
    "print(f\"\\n🎯 F1 Score on Validation Set (LDA): {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da893b6-57c6-4188-aeb2-c793b5e3a65b",
   "metadata": {},
   "source": [
    "## Non Linear Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b020c480-1be4-43d4-a675-44203b5381ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Define KNN model\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "# Hyperparameter grid for KNN\n",
    "param_grid = {\n",
    "    'n_neighbors': [1, 3, 5],\n",
    "    'metric': ['euclidean', 'minkowski', 'manhattan']\n",
    "}\n",
    "\n",
    "# Grid search with GroupKFold\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=knn_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=GroupKFold(n_splits=5),\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit model\n",
    "grid_search.fit(X_train_normalized, y_train, groups=X_train['sentence'])\n",
    "\n",
    "# Best estimator\n",
    "knn = grid_search.best_estimator_\n",
    "\n",
    "# Predict\n",
    "y_pred = knn.predict(X_val_normalized)\n",
    "\n",
    "# Probabilities (only works if n_neighbors > 1)\n",
    "y_probs = knn.predict_proba(X_val_normalized)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Test F1 Score:\", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a65b5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_model = MLPClassifier(random_state=42, max_iter=500)\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001],\n",
    "    'solver': ['adam']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=mlp_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=GroupKFold(n_splits=5),\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_normalized, y_train, groups=X_train['sentence'])\n",
    "\n",
    "best_mlp = grid_search.best_estimator_\n",
    "y_pred = best_mlp.predict(X_val_normalized)\n",
    "y_probs = best_mlp.predict_proba(X_val_normalized)[:, 1]\n",
    "\n",
    "print(\"Best Parameters (MLP):\", grid_search.best_params_)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Test F1 Score (MLP):\", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4e4cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "qda_model = QuadraticDiscriminantAnalysis()\n",
    "\n",
    "param_grid = {\n",
    "    'reg_param': [0.0, 0.1, 0.5]  # Regularization\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=qda_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=GroupKFold(n_splits=5),\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_normalized, y_train, groups=X_train['sentence'])\n",
    "\n",
    "best_qda = grid_search.best_estimator_\n",
    "y_pred = best_qda.predict(X_val_normalized)\n",
    "y_probs = best_qda.predict_proba(X_val_normalized)[:, 1]\n",
    "\n",
    "print(\"Best Parameters (QDA):\", grid_search.best_params_)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Test F1 Score (QDA):\", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808c4618",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "gnb_model = GaussianNB()\n",
    "\n",
    "# No real hyperparameters to tune in GNB, but wrap for consistency\n",
    "param_grid = {}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=gnb_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=GroupKFold(n_splits=5),\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_normalized, y_train, groups=X_train['sentence'])\n",
    "\n",
    "best_gnb = grid_search.best_estimator_\n",
    "y_pred = best_gnb.predict(X_val_normalized)\n",
    "y_probs = best_gnb.predict_proba(X_val_normalized)[:, 1]\n",
    "\n",
    "print(\"Best Parameters (GNB):\", grid_search.best_params_)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Test F1 Score (GNB):\", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f99ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier(class_weight='balanced', random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=dt_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=GroupKFold(n_splits=5),\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train_normalized, y_train, groups=X_train['sentence'])\n",
    "\n",
    "best_dt = grid_search.best_estimator_\n",
    "y_pred = best_dt.predict(X_val_normalized)\n",
    "y_probs = best_dt.predict_proba(X_val_normalized)[:, 1]\n",
    "\n",
    "print(\"Best Parameters (Decision Tree):\", grid_search.best_params_)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Test F1 Score (Decision Tree):\", f1_score(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0081b56a-1ed7-45d0-b398-1184027fd7f9",
   "metadata": {},
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b94bba-2fff-48dd-817e-5a0f788a4526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Initialize Random Forest with balanced class weights\n",
    "rf_model = RandomForestClassifier(\n",
    "    class_weight='balanced_subsample',  # Handles imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1  # Parallel processing\n",
    ")\n",
    "\n",
    "# Parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [200, 250],       # Number of trees\n",
    "    'max_depth': [20, 30, 50],      # Tree depth\n",
    "    'min_samples_split': [5, 10],      # Minimum samples to split\n",
    "    'min_samples_leaf': [6, 10],\n",
    "}\n",
    "\n",
    "# Group-aware GridSearchCV\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_model,\n",
    "    param_grid=param_grid,\n",
    "    cv=GroupKFold(n_splits=5),  # 10-fold grouped CV\n",
    "    scoring='f1',                # Focus on F1 for root class\n",
    "    n_jobs=-1,                   # Parallelize\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit (ensure X_train doesn't contain 'sentence' column)\n",
    "grid_search.fit(\n",
    "    X_train_normalized,  # Exclude group identifier\n",
    "    y_train,\n",
    "    groups=X_train['sentence']  # Grouping key\n",
    ")\n",
    "\n",
    "# Best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "\n",
    "# Predictions\n",
    "y_pred = best_rf.predict(X_val_normalized)\n",
    "y_probs = best_rf.predict_proba(X_val_normalized)[:, 1]\n",
    "\n",
    "# Evaluation\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print(\"Test F1 Score:\", f1_score(y_val, y_pred))\n",
    "\n",
    "# Feature Importance\n",
    "importances = best_rf.feature_importances_\n",
    "print(\"Feature Importances:\", dict(zip(X_train_normalized, importances)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82b7a89-f54f-4da8-870e-1b1bf3e222da",
   "metadata": {},
   "source": [
    "## Fit the Chosen Model on Entire Train Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0884d2d-66bb-4d5f-9d18-812f28026879",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_normalized = scaling(X).drop(['language', 'sentence', 'vertex'], axis=1)\n",
    "X_normalized.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e67cd6-8bf2-4886-b07a-1b25b7bd8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit whole data\n",
    "best_rf.fit(X_normalized, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f35362-cd91-4e63-aedf-6531568c814e",
   "metadata": {},
   "source": [
    "## Make Predictions on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2051014-e9b3-4be6-8bcd-f67c33e2d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test data\n",
    "test_df = pd.read_csv('test.csv')\n",
    "X_test = preprocess(test_df)\n",
    "test_normalized = scaling(X_test).drop(['language', 'sentence', 'vertex'], axis=1)\n",
    "\n",
    "# Predictions\n",
    "test_pred = best_rf.predict(test_normalized)\n",
    "test_probs = best_rf.predict_proba(test_normalized)[:, 1]\n",
    "\n",
    "X_test['probability'] = test_probs\n",
    "\n",
    "df_max = X_test.loc[X_test.groupby(['language', 'sentence'])['probability'].idxmax()]\n",
    "\n",
    "df_max = df_max[['language','sentence', 'vertex']].rename(columns={'vertex': 'root'}).reset_index(drop=True)\n",
    "\n",
    "\n",
    "submission_df = test_df.merge(df_max, on=['language', 'sentence'], how='left')\n",
    "submission_df = submission_df[['id', 'root']]\n",
    "\n",
    "submission_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81f994d",
   "metadata": {},
   "source": [
    "Feature Importance for Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80c234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Get feature importances and names\n",
    "importances = best_rf.feature_importances_\n",
    "feature_names = test_normalized.columns  # Make sure to match this with your training feature names\n",
    "\n",
    "# Create a DataFrame for better plotting\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))  # Show top 20\n",
    "plt.title('Top Feature Importances (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60fc22ec-1ce4-4fab-bd8e-3bd5f1dae2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission_df.to_csv('submission_randomforest_features.csv', index=False)\n",
    "print(\"Submission file created: submission_randomforest_features.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94638474-daeb-43f4-a62d-9847eff9a179",
   "metadata": {},
   "source": [
    "accuracy shouldnt be used as a metric when there is class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd1b275-a6c9-4778-a234-02755541641b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
